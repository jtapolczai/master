\documentclass[bibliography=totoc ]{scrartcl}

\usepackage[usenames, dvipsnames]{xcolor}

\definecolor{PaleRed}{RGB}{202,61,58}
\definecolor{DeepRed}{RGB}{158,46,43}
\definecolor{PaleBlue}{RGB}{100,180,250}
\definecolor{DeepBlue}{RGB}{30,100,255}
\definecolor{DeepGreen}{RGB}{60,150,30}
\definecolor{LightGray}{RGB}{220,220,220}

\usepackage[a4paper, left=40mm, right=40mm, top=50mm, bottom=40mm]{geometry}
\usepackage[colorlinks=true, linkcolor=DeepBlue, citecolor=DeepGreen]{hyperref}
\usepackage{url}

\def\UrlBreaks{\do\/\do-}

\title{{\huge Design and implementation of a biologically inspired agent architecture combining emotions and reasoning}}
\author{Janos Tapolczai\\
Thesis supervisor: A.o. Univ.-Prof. Dr. Hans Tompits}

\begin{document}

\maketitle

\section{Problem statement}

To date, emotions in AI have very much occupied a niche; instead, problems like search, reasoning, pattern recognition and statistics have dominated the field. People like Marvin Minsky and Aaron Sloman, however, have argued that emotions play a key role in cognition --- to wit, Minsky's famous saying \cite[p. 163]{societyOfMind}:

\begin{quote}
The question is not whether intelligent machines can have any emotions, but whether machines can be intelligent without any emotions.
\end{quote}

One of the major unsolved problems --- in biology as well as AI --- is the interaction between emotions and reasoning: how do they help and how do they hinder the reasoning process? Could reasoning function without emotions? Are they just an external influence, or an integral part of the reasoning process?

This thesis would build primarily on Minsky's \cite{societyOfMind, emotionMachine} and Sloman's~\cite{slomanNiche} work, who seek to answer these questions with the help of evolutionary neuroscience: they only consider architectures that could have evolved gradually. By thus constraining their models through the criterion of evolvability, they sketch plausible histories through which the human brains might have gone, accruing one subsystem at a time. The goal of this work would be the application of their method to affect and reasoning specifically, and to the development of a cognitive architecture that incorporates both.

\section{Expected results}

%The goal of this project is to strike a pragmatic middle ground between the biologically inspired and the reasoning-driven approaches, with the aim of inching closer towards the kind of general, human-level AI to which people like Minsky, McCarthy, and Sloman aspire \cite{emotionMachine, mcCarthy2007, minsky2004}.
The primary aim is the creation of an affective toy-AI application in which agents have to navigate in a simple, competitive world and in which they do not perform reasoning as such, but using emotions and a rudimentary prediction of the future to choose their actions. Its architecture would be hard-wired, but it would be parametrisable via its emotional reactions to stimuli: whereas one agent might react to the sight of an another agent with anger, a differently parametrised agent might react with fear.

The expectation is that these agents will be able to effectively navigate said toy world which, to a rough approximation, resembles the typical environment of animals: competition for food, cooperation and conflict with other, similar agents, and reproduction. If the toy AI and its underlying architecture work as intended, we should see patterns of behaviour similar to those we find in real organisms: some would be overly frightful, fleeing from every threat; some would react aggressively to almost anything; yet others would act out an iterated prisoner's dilemma in which they would slowly become more cooperative with agents with whom they had positive interactions in the past, yet would quickly develop a lasting aversion if a negative interaction occurred.

This test scenario would (a) determine how successful the overall model is and (b) how successful different parameter sets with respect to emotional reactions within the confines of the model are. If at least some subset of agents can keep itself from extinction, we would have an indication that their behaviour at least matches that of some lower animal in terms of effectiveness.

In case of a success, we would have a proof-of-concept that the evolutionary approach is workable and useful in the design of AIs, and that this sort of ``neurological archaeology'' or constructing cognitive architectures by analysing evolutionary steps of real brains has its place besides traditional approaches like neural networks and symbolic computation.

\section{Methodology}

The thesis would be split into two parts: a theoretical and a practical one. The first, theoretical part would analyse nervous systems and brain structures; the second, practical part propose a cognitive architecture based on the first and on the work of Aaron Sloman. The basis of the entire work would be an evolutionary model: with the help of biology, we would look at each subsystem in the brain --- how it developed, on what other subsystem it relied, what analogues exist in non-human species, how it interacts with other subsystems --- and consequently develop a cognitive architecture.

The underlying hypothesis is that, since brains are evolved organs, their functioning can best be understood in terms of re-tracing their development through various stages. By requiring each development in the brain function to be relatively simple and useful by itself, we can constrain the space of possible designs. This approach of design space and (evolutionary) niche space was, as said above, proposed by Aaron Sloman \cite{slomanNiche} and is a running theme in many of his works \cite{sloman2000, sloman1993, sloman1997}.

The interaction between affect and reasoning being the focus of this work, re-tracing brain development would involve giving an account of the following:

\begin{enumerate}
	\item the structure and purpose of affect;
	\item the purpose and mechanism of planning; and
	\item the structure of reasoning, as it relates to the above two, and as humans actually perform it.
\end{enumerate}

The second, practical part, would serve as a test for the developed model. Here, we use simulation as a way of evaluation. The aim is to develop an AI that, in simple model worlds, can perform with comparable effectiveness as animals do in the real one. Hence, the agents would be tested in a toy world that poses the typical challenges that animals face (in an extremely simplified form, of course):

\begin{enumerate}
	\item hunger, which necessitates the gathering of resources,
	\item predators (agents with a simple ``flight-or-fight'' AI),
	\item plants that can be eaten, and
	\item other, similar agents, with which one can interact in hostile or cooperative ways.
\end{enumerate}
Successful agents would reproduce and their offspring would inherit their emotional profiles. To allow agents with different personalities to better differentiate themselves from others, the protocol of interaction would be minimalistic: an agent can outright attack another one, give it some food item, or send a gesture (a string) of its choosing. It would be incumbent upon the recipient of such gestures to interpret them in accordance with its emotional profile --- there would be no a priori knowledge about which gestures meant what; two different agents could interpret the same signal in completely different ways.

As underlying reasoning engine of the architecture, we plan to use the DLVHEX system~\cite{dlvhex}, which is an extension of the state-of-the art answer-set solver DLV~\cite{dlv}, allowing an interface to external sources --- in particular, in our case, providing the necessary interface between the emotional model and the reasoning functionality.


 
\section{State of the art}

Research into human-level AI is proceeding less fast as was originally hoped in the early days of AI. Although AI systems can outperform humans at a variety of tasks (notably games like chess and bridge \cite{Schaeffer00} that are amenable to standard search techniques like A* and IDDFS, but also complex, real-world problems like driving cars \cite{googleCar, googleCar2} or reasoning \cite{asp1, acthex}), an artificial intelligence that can compete with a human at many different tasks has yet to be created. While most effort today goes into the creation of such weak AIs, there are a few influential people --- Marvin Minsky, John McCarthy, Nils Nilsson, and Patrick Winston, among others \cite[p. 27]{norvig} --- who wish to create programs that act in a believably human-like way.

On the other hand, much research has been done in the field of affective neuroscience and about the basis of emotions (for an overview, see {\em The Cambridge Handbook of Human Affective Neuroscience} \cite{cambridgeAff}). These findings have been applied to artificial intelligence to some degree --- examples being Cynthia Breazeal's work at MIT \cite{kismet, breazeal2003}, and the emotion-driven robots of Sandra Gadanho and John Hallam \cite{DBLP:journals/adb/GadanhoH01}.

A number of cognitive architectures do exist, from the purely theoretical~\cite{emotionMachine} to those actually implemented, such as CMU's 4CAPS \cite{4caps} and those based on R. A. Brooks' subsumption architecture \cite{brooksSubsumption}. In {\em Intelligence without Reason}~\cite{Brooks91intelligencewithout}, Brooks does, in fact, come quite close to our goal: a biologically inspired architecture that does not have a priori knowledge about reasoning, but merely synthesizes it from the interplay of simpler components.

\section{Context}

This thesis deals with affective AI; as such, it relates very closely to the curriculum of Computational Intelligence.

% \pagebreak

\bibliographystyle{plain}
\bibliography{proposal}

\begin{minipage}[t][1.5cm][t]{\textwidth}%
  \vspace{20pt}
  \begin{tabbing}%
    \= \hspace{63mm} \= \hspace{51mm} \kill
    \> %{\raggedright\rule{51mm}{0.5pt}} 
\> {\raggedright\rule{51mm}{0.5pt}} \\
    \> 
% \begin{minipage}[t][0.5cm][t]{51mm}\centering (Signature of Author)\end{minipage}
    \> \begin{minipage}[t][0.5cm][t]{51mm}\centering (Signature of Advisor)\end{minipage}
    \end{tabbing}
\end{minipage}

\end{document}

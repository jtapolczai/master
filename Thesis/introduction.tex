The history of AI is marked by vacillations between two paradigms: the biological and the ideal one. These sometimes move closer to each other, and then apart again, due to the fact that they are naturally in tension, yet also inextricably bound together. The biological model, which subsumes the connectionist and the cybernetic ones, seeks to imitate biological organisms on the lowest level; cybernetics, now unpopular, tried to emulate intelligence via feedback loops; connectionism wants to  build complex cognition out of interconnected, simple parts. Its most famous instance --- neural networks --- and the approach in general, have, time and time again, failed to produce even the rudimentary intelligence we seen in animals. The high hopes of the pioneers of the field that the mathematical modelling of some primitive, biologically inspired computing machinery, cleverly pieced together, could mimic the cognition of humans, were sorely disappointed. Then, after years of lacklustre results, Marvin Minsky published his devastating proof of the theoretical weakness of perceptrons --- a then popular type of neural network --- in his book {\em Perceptrons: An Introduction to Computational Geometry} \cite{Minsky1988}. This led to the abandonment of the approach among AI researchers and, arguably, to the AI winter of the 1970s. In hindsight, we can say that, despite their unimpeachable brilliance, their position in history imposed upon them a fatal naivete and an optimism born from the ignorance of the harrowing complexity of an organ that had been, in one form or another, a good few hundred million years in the making. The idea that neural networks mimicking the smallest-scale structures in the brain and consisting of few thousand nodes, trained over the course of minutes or days, could hope to emulate human intelligence, turned out to be false.

Though it had been developed coterminously with the connectionist approach, the ideal approach came to prominence after the former's disappointments. We can identify it largely with {\em symbolic} computation, whose proponents falls into the two camps (allegedly first identified by Roger Schank): the {\em neats}, who favour provably correct methods, and the {\em scruffies}, who are willing to use whatever works \cite[pp. 421â€“424]{McCorduck2004}. Both, to some degree, kept a penchant for mathematical formalisms, but they abandoned the goal of re-tracing the workings of biological brains. The neats especially, in an Aristotelian manner, tried to automate how one {\em ought to think}. How humans accomplished their tasks was no longer of importance; only what they accomplished was --- and preferably, that {\em what} was abstract reasoning. Out of this school thought came many applications that have proved very useful: automated planning, in the forward and backward variety; default and common-sense reasoning; answer-set and logic programming; Prolog; knowledge-based systems that assist experts in their decision-making, in a way, informed an uninformed search algorithms. Many of these can now outperform the best humans in specialized tasks. Knowledge-based in medicine now rival doctors in the quality of their diagnoses \cite{bla}. Automated driving system can navigate through traffic accident-free \cite{googleCar}. In chess, the greatest human players have trouble keeping up with computers, as Gary Kasparov famous loss to Deep Blue showed in 1997 \cite{deepBlue}. Yet, impressive as they are --- and they are impressive ---, these {\em weak AIs} have not allowed us to piece together the big picture.\footnote{As a side note, it should be said that that the term ``weak AI'' may be unfairly denigrating. Even if we came into possession of human-level strong AI, so-called weak  AIs would still easily outperform it in special areas, just as they outperform humans today.} As good as they are, one would be hard-pressed to call them intelligent in the colloquial sense. As good as any chess program or automated car is, they are still just machines (again in the colloquial sense). It would indeed be absurd to propose that one could exchange one for the other. While Gary Kasparov was able to drive home after his match against IBM's Deep Blue, the notion of ``car'' was not even part of his opponent's conceptual universe. Equally, the control system in Google's self-driving car would not be able to play chess, or assemble a shopping list, read a book, smell, interpret emotions, or do any number of things of which humans and animals are capable.

In recent years, the schism between the biological/connectionist and the ideal/symbolic schools of thought has grown quite pronounced, they are, on a deeper level, joined at the hip. The simple reason for this is that the brain is both a ``messy'' biological organ, and a high-level computation device. It is neither an amorphous mass of neurons, as connectionism models it, nor is it merely a neutral hardware for idealized reasoning, to which the symbolic approach is wont to reduce it. It is both, and in this thesis, I submit that any strong AI must find find a bridge between the two views and integrate them into one whole.

This thought is not new; such a bridge already exists in the form of the {\em integrated approaches}. This vague category comprises things like James Albus's Hierarchical Control Systems \cite{albusHCS}, Rodney Brook's Subsumption Architectures \cite{brooksSubsumption}, Carnegie-Mellon's 4CAPS \cite{4caps}, and, in theory, the architecture laid out in Minsky's The Emotion Machine \cite{emotionMachine}. The chief commonality of these systems is that they proceed from an engineering perspective, not from the mathematical one of symbolic computation, or the biological one of connectionism. Yet, they attempt to combine the high-quality results of the mathematical methods with the dynamism of the biologically inspired systems. It is to this category to which I hope to make a small contribution by proposing that one must not only consider the mind not just as messy, but something worse: as {\em evolved}. I submit that we can best make sense of the seemingly random jumble of features, defects, idiosyncrasies, and quirks of brains by tracing their history. By going through the developments step-by-step, by proposing simple components that came about one by one and grew over time, what is seemingly random and senseless can begin to make sense. By re-creating them, we can hope to create {\em authentic} intelligences --- ones that match biological ones not just in raw processing power, but also in kind.

\paragraph{Structure of thesis.} After Section~\ref{sec:relatedWork} \textbf{Related work}, the thesis consists of two large segments. The first one is the theoretical argument and empirical data supporting it. It deals with the origin and purpose of neural systems, their evolution (insofar as is known), the components of which they are likely comprised, and how these could have come about. The constituent sections are
	\begin{itemize}
		\item Section~\ref{sec:preliminaries} \textbf{Preliminary considerations}, containing the general evolutionary story,
		\item and Section~\ref{sec:schemaOfCognition} \textbf{Schema of cognition}, which hypothesizes about the cognitive structure of humans.
		\item After that, Section~\ref{sec:mathematicalModel} \textbf{Mathematical notation}, in which we introduce a bit of notation to ease talking about the systems, and
		\item Section~\ref{sec:selectedSubsystems} \textbf{Selected subsystems}, in which a number of proposed subsystems are sketched.
	\end{itemize}
	
The second segment puts the material of the first into practice and consists of Section~\ref{sec:proposedArchitecture} \textbf{Proposed architecture} and Section~\ref{sec:implementation} \textbf{Implementation}. Therein, I describe a kind of affective agent architecture that utilizes both emotions and reasoning to navigate a toy world populated by others like it. The last section contains the result of that experiment.

I would like to remark that everything in this work is, at best, a rough outline;: though (conjectured to be) basically correct, and, I hope, useful, we do simply not have the huge amount of data about real brains to construct truly faithful models of them, or to verify even hypothesis in detail.
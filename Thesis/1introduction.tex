The history of AI is marked by vacillations between two paradigms: the biological and the ideal one. The biological school of thought subsumes ideas like connectionism, which envisions the mind as an interconnected system of simple components, generally single neurons, and tries to build AIs via neural networks. Opposed to this view stand those schools that view the mind as an abstract machine: computationalism holds it to be an information processing system that deals in the manipulation of symbols, and which possesses structures like sub-systems, rules, and syntax.

In this work, we shall build upon this latter, computationalist approach and, more specifically, upon the work of Marvin Minsky and Aaron Sloman, who have very much advocated the idea of the mind as a control system with an intelligible structure in books like {\em The Emotion Machine} \cite{emotionMachine}, {\em Society of Mind} \cite{societyOfMind}, {\em The Mind as a Control System} \cite{sloman1993}, and {\em What Sort of Control System Is Able to Have a Personality?} \cite{sloman1997}. One of the running themes in Minsky's and Sloman's work is the criterion of evolvability: it is no good, they argue, to merely propose some ideal reasoning apparatus; if AIs faithful to their biological inspirations are to be constructed, we must structure them as biological minds are structured --- and that is best accomplished by thinking about what sorts of subsystems might have evolved in what order, in what way, and for what task. The human-level AI must therefore replicate the brain's functions, warts included.

In the rest of this thesis, we shall pursue this idea, with special attention given to the interaction between emotions and reasoning in the sense of logical deduction. The result will be a small cognitive architecture that combines both, but privileging neither. Both Sloman and Minsky have sketched such architectures in the past, and ours will be similar to these in its broad outlines; however, we add two new contributions:

\begin{enumerate}
	\item first, the ``how'' of the evolution of sub-systems is investigated in greated detail. We do not just ask in what order capabilities like pain, anger, deduction, or introspection evolved, but how they could have come about. Were they simply re-purposed from other, existing components, or entirely new creations?
	\item Second, we look at the interactions between sub-systems. How could something like a discrete sub-system even develop? In what manner do different ones communicate? Is there some universal, perhaps symbolic, communication protocol --- or a suite of protocols?
\end{enumerate} 

\paragraph{White-box model} These how-questions will make up the first, larger part of the thesis, wherein we propose that the brain is a fundamentally chaotic entity. Although it evidently has large-scale structure, it possesses a very much different computational model than that seen in conventional programming languages, if one can call it a computational model at all. It might be most appropriate to term this a {\em white-box model of cognition}: whereas the most important unit in structured programming is the function, which works like a black box that receives parameters and returns a result, there are no such barriers in the brain. Rather, everything is done ``in the open'': neurons carry signals from different parts of an organism, and between themselves. Groups of neurons might amplify, invert, swallow the signals sent out by others, and their signalling might in turn by modulated by other groups of neurons. If newly developing functionality is to make use of pre-existing one, it does not have call it in a well-defined way, and neither does it have to register itself as one would register a plug-in in human-made software. It can simply interpose itself and listen in on the neural activity, and send out signals that mimic those to which older component has evolved to respond. Although be so closely connected that they're physically intertwined, the manner in which they communicate is, we propose, akin to a loosely coupled software system or a publish/subscribe-architecture in which different actors know little of each other and send out messages without knowing who else might read them.

\paragraph{Reasoning and emotions} After this groundwork, we will come back to the large-scale systems, specifically imagination, its relationship to affect, and reasoning. We contend that imagination --- perceiving events that are not happening --- is the antecedent of abstract reasoning, and their both were gradually evolved from older functionality, rather either being sui generis.



\vspace{3cm}

The contribution of this thesis will be two-fold: first, we apply this evolutionary reasoning to the ``how''. The question will not only be in which order capabilities
the application of this line of reasoning to the ``how'': we ask not just in what order capabilities like emotions, reasoning, or consciousness evolved, but 

In this thesis, we shall take this view to heart, and make a new contribution by applying to, not just to large sub-systems, but to the lowest levels of neural sys

The overall architecture proposed will not be new, as these two, along with others, have laid solid foundations over the years, but we will contribute by taking their ideal and applying them to lowest levels of neural systems 

The history of AI is marked by vacillations between two paradigms: the biological and the ideal one. These sometimes move closer to each other, and then apart again, due to the fact that they are naturally in tension, yet also inextricably bound together. The biological model, which subsumes the connectionist and the cybernetic ones, seeks to imitate biological organisms on the lowest level; cybernetics, now unpopular, tried to emulate intelligence via feedback loops; connectionism wants to  build complex cognition out of interconnected, simple parts.

In this work, we will purse this connectionist approach




Its most famous instance --- neural networks --- and the approach in general, have, time and time again, failed to produce even the rudimentary intelligence we seen in animals. The high hopes of the pioneers of the field that the mathematical modelling of some primitive, biologically inspired computing machinery, cleverly pieced together, could mimic the cognition of humans, were sorely disappointed. Then, after years of lacklustre results, Marvin Minsky published his devastating proof of the theoretical weakness of perceptrons --- a then popular type of neural network --- in his book {\em Perceptrons: An Introduction to Computational Geometry} \cite{Minsky1988}. This led to the abandonment of the approach among AI researchers and, arguably, to the AI winter of the 1970s. In hindsight, we can say that, despite their unimpeachable brilliance, their position in history imposed upon them a fatal naivete and an optimism born from the ignorance of the harrowing complexity of an organ that had been, in one form or another, a good few hundred million years in the making. The idea that neural networks mimicking the smallest-scale structures in the brain and consisting of few thousand nodes, trained over the course of minutes or days, could hope to emulate human intelligence, turned out to be false.

Though it had been developed coterminously with the connectionist approach, the ideal approach came to prominence after the former's disappointments. We can identify it largely with {\em symbolic} computation, whose proponents falls into the two camps (allegedly first identified by Roger Schank): the {\em neats}, who favour provably correct methods, and the {\em scruffies}, who are willing to use whatever works \cite[pp. 421â€“424]{McCorduck2004}. Both, to some degree, kept a penchant for mathematical formalisms, but they abandoned the goal of re-tracing the workings of biological brains. The neats especially, in an Aristotelian manner, tried to automate how one {\em ought to think}. How humans accomplished their tasks was no longer of importance; only what they accomplished was --- and preferably, that {\em what} was abstract reasoning. Out of this school thought came many applications that have proved very useful: automated planning, in the forward and backward variety; default and common-sense reasoning; answer-set and logic programming; Prolog; knowledge-based systems that assist experts in their decision-making, in a way, informed an uninformed search algorithms. Many of these can now outperform the best humans in specialized tasks. Knowledge-based in medicine now rival doctors in the quality of their diagnoses \cite[p. 592, Table 31-1]{mycin}. Automated driving system can navigate through traffic accident-free \cite{googleCar}. In chess, the greatest human players have trouble keeping up with computers, as Gary Kasparov famous loss to Deep Blue showed in 1997 \cite{deepBlue}. Yet, impressive as they are --- and they are impressive ---, these {\em weak AIs} have not allowed us to piece together the big picture.\footnote{As a side note, it should be said that that the term ``weak AI'' may be unfairly denigrating. Even if we came into possession of human-level strong AI, so-called weak  AIs would still easily outperform it in special areas, just as they outperform humans today.} As good as they are, one would be hard-pressed to call them intelligent in the colloquial sense. As good as any chess program or automated car is, they are still just machines (again in the colloquial sense). It would indeed be absurd to propose that one could exchange one for the other. While Gary Kasparov was able to drive home after his match against IBM's Deep Blue, the notion of ``car'' was not even part of his opponent's conceptual universe. Equally, the control system in Google's self-driving car would not be able to play chess, or assemble a shopping list, read a book, smell, interpret emotions, or do any number of things of which humans and animals are capable.

In recent years, the schism between the biological/connectionist and the ideal/symbolic schools of thought has grown quite pronounced, they are, on a deeper level, joined at the hip. The simple reason for this is that the brain is both a ``messy'' biological organ, and a high-level computation device. It is neither an amorphous mass of neurons, as connectionism models it, nor is it merely a neutral hardware for idealized reasoning, to which the symbolic approach is wont to reduce it. It is both, and in this thesis, I submit that any strong AI must find find a bridge between the two views and integrate them into one whole.

This thought is not new; such a bridge already exists in the form of the {\em integrated approaches}. This vague category comprises things like James Albus's Hierarchical Control Systems \cite{albusHCS}, Rodney Brook's Subsumption Architectures \cite{brooksSubsumption}, Carnegie-Mellon's 4CAPS \cite{4caps}, and, in theory, the architecture laid out in Minsky's The Emotion Machine \cite{emotionMachine}. The chief commonality of these systems is that they proceed from an engineering perspective, not from the mathematical one of symbolic computation, or the biological one of connectionism. Yet, they attempt to combine the high-quality results of the mathematical methods with the dynamism of the biologically inspired systems. It is to this category to which I hope to make a small contribution by proposing that one must not only consider the mind not just as messy, but something worse: as {\em evolved}. I submit that we can best make sense of the seemingly random jumble of features, defects, idiosyncrasies, and quirks of brains by tracing their history. By going through the developments step-by-step, by proposing simple components that came about one by one and grew over time, what is seemingly random and senseless can begin to make sense. By re-creating them, we can hope to create {\em authentic} intelligences --- ones that match biological ones not just in raw processing power, but also in kind.

\paragraph{Structure of thesis.} After Section~\ref{sec:relatedWork} \textbf{Related work}, the thesis consists of two large segments. The first one is the theoretical argument and empirical data supporting it. It deals with the origin and purpose of neural systems, their evolution (insofar as is known), the components of which they are likely comprised, and how these could have come about. The constituent sections are
	\begin{itemize}
		\item Section~\ref{sec:preliminaries} \textbf{Preliminary considerations}, containing the general evolutionary story,
		\item and Section~\ref{sec:schemaOfCognition} \textbf{Schema of cognition}, which hypothesizes about the cognitive structure of humans.
		\item After that, Section~\ref{sec:mathematicalModel} \textbf{Mathematical notation}, in which we introduce a bit of notation to ease talking about the systems, and
		\item Section~\ref{sec:selectedSubsystems} \textbf{Selected subsystems}, in which a number of proposed subsystems are sketched.
	\end{itemize}
	
The second segment puts the material of the first into practice and consists of Section~\ref{sec:proposedArchitecture} \textbf{Proposed architecture} and Section~\ref{sec:implementation} \textbf{Implementation}. Therein, I describe a kind of affective agent architecture that utilizes both emotions and reasoning to navigate a toy world populated by others like it. The last section contains the result of that experiment.

I would like to remark that everything in this work is, at best, a rough outline;: though (conjectured to be) basically correct, and, I hope, useful, we do simply not have the huge amount of data about real brains to construct truly faithful models of them, or to verify even hypothesis in detail.